{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    " \n",
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT ENCODING\n",
    "\n",
    "# Loading dataset\n",
    "dataset = pd.read_csv(\"./Data/CSV_format/prose_dialogues_recursive.csv\") \n",
    "input = dataset['INPUT'].values.tolist()\n",
    "target = dataset['TARGET'].values.tolist()\n",
    "\n",
    "# Loading tokenizer\n",
    "TOKENIZER = CamembertTokenizer.from_pretrained(\n",
    "    'camembert-base',\n",
    "    do_lower_case=True)\n",
    "\n",
    "# Encoding a batch of data\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(input,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=MAX_LENGTH,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')\n",
    " \n",
    "# Convert target into tensors\n",
    "targets = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "split_border = int(len(targets)*0.8)\n",
    " \n",
    " \n",
    "train_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][:split_border],\n",
    "    encoded_batch['attention_mask'][:split_border],\n",
    "    targets[:split_border])\n",
    "validation_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][split_border:],\n",
    "    encoded_batch['attention_mask'][split_border:],\n",
    "    targets[split_border:])\n",
    " \n",
    " \n",
    "batch_size = 32\n",
    " \n",
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    " \n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler = SequentialSampler(validation_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE MODEL\n",
    "\n",
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8) # Epsilon\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "# On va stocker nos tensors sur mon cpu : je n'ai pas mieux\n",
    "device = torch.device(\"cpu\")\n",
    " \n",
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []\n",
    " \n",
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    " \n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()\n",
    " \n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    " \n",
    "        # On fait un print chaque 40 batchs\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print(f'  Batch {step}  of \n",
    "{len(train_dataloader)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        sentiment = batch[2].to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()        \n",
    " \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        loss, logits = model(input_id, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=sentiment)\n",
    " \n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss.item()\n",
    " \n",
    "        # Backpropagtion\n",
    "        loss.backward()\n",
    " \n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    " \n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))  \n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"Model saved!\")\n",
    "torch.save(model.state_dict(), \"./sentiments.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSEMENT\n",
    "\n",
    "def preprocess(raw_reviews, sentiments=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                truncation=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if sentiments:\n",
    "        sentiments = torch.tensor(sentiments)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    " \n",
    "def predict(reviews, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "         \n",
    "        return torch.argmax(retour[0], dim=1)\n",
    " \n",
    " \n",
    "def evaluate(reviews, sentiments):\n",
    "    predictions = predict(reviews)\n",
    "    print(metrics.f1_score(sentiments, predictions, average='weighted', zero_division=0))\n",
    "    seaborn.heatmap(metrics.confusion_matrix(sentiments, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "011b5ae7b1f824e9e17532cf660bf76a4e5d222979bbdbc9a307afdbcbfac48c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
