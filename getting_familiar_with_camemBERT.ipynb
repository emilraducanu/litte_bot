{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    " \n",
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85368/1645088141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Encoding a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m encoded_batch = TOKENIZER.batch_encode_plus(input,\n\u001b[0m\u001b[1;32m     16\u001b[0m                                             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2684\u001b[0m         )\n\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2687\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    714\u001b[0m                     \u001b[0;34m\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "# TEXT ENCODING\n",
    "\n",
    "# Loading dataset\n",
    "dataset = pd.read_csv(\"./Data/CSV_format/prose_dialogues_recursive.csv\") \n",
    "input = dataset.INPUT.tolist()\n",
    "target = dataset.TARGET.tolist()\n",
    "\n",
    "# Loading tokenizer\n",
    "TOKENIZER = CamembertTokenizer.from_pretrained(\n",
    "    'camembert-base',\n",
    "    do_lower_case=True)\n",
    "\n",
    "# Encoding a batch of data\n",
    "MAX_LENGTH = 300\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(input,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=MAX_LENGTH,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')\n",
    " \n",
    "# Convert target into tensors\n",
    "targets = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "split_border = int(len(targets)*0.8)\n",
    " \n",
    " \n",
    "train_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][:split_border],\n",
    "    encoded_batch['attention_mask'][:split_border],\n",
    "    targets[:split_border])\n",
    "validation_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][split_border:],\n",
    "    encoded_batch['attention_mask'][split_border:],\n",
    "    targets[split_border:])\n",
    " \n",
    " \n",
    "batch_size = 32\n",
    " \n",
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    " \n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler = SequentialSampler(validation_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE MODEL\n",
    "\n",
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8) # Epsilon\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "# On va stocker nos tensors sur mon cpu : je n'ai pas mieux\n",
    "device = torch.device(\"cpu\")\n",
    " \n",
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []\n",
    " \n",
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    " \n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()\n",
    " \n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    " \n",
    "        # On fait un print chaque 40 batchs\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print(f'  Batch {step}  of \n",
    "{len(train_dataloader)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        sentiment = batch[2].to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()        \n",
    " \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        loss, logits = model(input_id, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=sentiment)\n",
    " \n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss.item()\n",
    " \n",
    "        # Backpropagtion\n",
    "        loss.backward()\n",
    " \n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    " \n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))  \n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"Model saved!\")\n",
    "torch.save(model.state_dict(), \"./sentiments.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSEMENT\n",
    "\n",
    "def preprocess(raw_reviews, sentiments=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                truncation=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if sentiments:\n",
    "        sentiments = torch.tensor(sentiments)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    " \n",
    "def predict(reviews, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "         \n",
    "        return torch.argmax(retour[0], dim=1)\n",
    " \n",
    " \n",
    "def evaluate(reviews, sentiments):\n",
    "    predictions = predict(reviews)\n",
    "    print(metrics.f1_score(sentiments, predictions, average='weighted', zero_division=0))\n",
    "    seaborn.heatmap(metrics.confusion_matrix(sentiments, predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "011b5ae7b1f824e9e17532cf660bf76a4e5d222979bbdbc9a307afdbcbfac48c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
